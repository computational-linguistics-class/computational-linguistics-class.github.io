---
layout: default
img: bert.png
caption: Models like BERT and GPT-2 are fun!
img_link: https://muppet.fandom.com/wiki/Bert  
title: Language Model Fine-Tuning
active_tab: homework
release_date: 2020-04-22
due_date: 2020-04-29T11:59:59EDT
submission_link: 
materials:
    - 
      name: Google Colab Notebook
      url: https://colab.research.google.com/drive/1HlY1rdEKqMFLtMSMcxgM3gKiQkgFxPY1
    - 
      name: Text Adventures Train Set
      url: http://http://computational-linguistics-class.org/homework/transformers/text_adventures_train.txt
    - 
      name: Text Adventures Development Set
      url: http://http://computational-linguistics-class.org/homework/transformers/text_adventures_dev.txt
    - 
      name: Text Adventures Test Set
      url: http://http://computational-linguistics-class.org/homework/transformers/text_adventures_test.txt

attribution: This assignment was created by Daphne Ippolito for UPenn's "Interactive Fiction and Text Generation" course in Spring 2020.  It was adapted for UPenn's CIS 530 course by Arun Kirubarajan, Tatiana Tsygankova, Sihao Chen, and Chris Callison-Burch in Spring 2020 during the Coronavirus pandemic.
readings:
- 
  title: Language Models are Unsupervised Multitask Learners
  authors: Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 
  venue: OpenAI Blog
  type: paper
  url: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
  abstract: Natural language processing tasks, such as ques-tion answering, machine translation, reading com-prehension,  and  summarization,  are  typicallyapproached  with  supervised  learning  on  task-specific datasets. We demonstrate that languagemodels begin to learn these tasks without any ex-plicit supervision when trained on a new datasetof millions of webpages called WebText. Whenconditioned on a document plus questions, the an-swers generated by the language model reach 55F1 on the CoQA dataset - matching or exceedingthe performance of 3 out of 4 baseline systemswithout  using  the  127,000+  training  examples.The capacity of the language model is essentialto the success of zero-shot task transfer and in-creasing it improves performance in a log-linearfashion across tasks. Our largest model, GPT-2,is a 1.5B parameter Transformer that achievesstate of the art results on 7 out of 8 tested lan-guage modeling datasets in a zero-shot settingbut still underfits WebText.   Samples from themodel reflect these improvements and contain co-herent paragraphs of text. These findings suggesta promising path towards building language pro-cessing systems which learn to perform tasks fromtheir naturally occurring demonstrations.
-
   title: BERT&colon; Pre-training of deep bidirectional transformers for language understanding
   authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
   venue: NAACL
   type: paper
   url: https://www.aclweb.org/anthology/N19-1423/
   abstract: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
---

<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this top secret assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->

<div class="alert alert-info">
This assignment is due before {{ page.due_date | date: "%I:%M%p" }} on {{ page.due_date | date: "%A, %B %-d, %Y" }}.
</div>

Large Pre-Trained Language Models<span class="text-muted">: Assignment 12</span>
=============================================================

<div class="alert alert-danger">
Warning: this top secret assignment is not ready yet.  It may still have updates before it is released.  Check with your instructor before you start working on this assignment.
</div>

## Introduction

For this homework, we will combine ideas from the entire course: language models, deep word representations, and neural networks.

The current state-of-the-art models for a variety of natural language processing tasks belong to the **Transformer Family**, which are models based off of the Transformer architecture. The Transformer can be thought of as a big feed-forward network, with some fancy bells and whistles such as the attention mechanism. You might be wondering: why are we moving back to serial networks after such successes with RNNs and LSTMs? It turns out that although recurrent models are naturally poised to handle sequences as inputs, their non-serial nature makes them difficult to train in a distributed/parallel fashion. This means that serial networks can be trained faster, unlocking new orders of magnitude of data to use as training data. Some examples of notable state-of-the-art Transformer based models are Google's BERT, and in this homework, Open AI's GPT-2. 

## OpenAI’s GPT Model
OpenAI experimented with removing the encoder part of the network, leaving a decoder-only model. In 2018, they released a very impressive language model named **GPT**, which stands for *Generative Pre-Training* as the model makes heavy use of **transfer learning**. Recall that transfer learning is using knowledge gained from one problem (or training setting), and applying it to another area or domain. In 2019, OpenAI released the model’s scaled-up **1.5 billion parameter** successor: **GPT-2**.  To create such a large model, OpenAI crawled and released a new dataset named **WebText**, which consists of 40 GB of high-quality, human verified text. The researchers also utilized efficient representation schemes such as Byte-Pair Encoding, which serves as a middle-layer between word-level and character-level representations. The result is an extremely impressive language model - so good that OpenAI didn’t release the trained model along with the paper in fear of individuals using it for malicious purposes.

### Generation Ability

Here are some examples of it’s generation, taken from [OpenAI’s release post](https://openai.com/blog/better-language-models/):

**Human-Written Prompt:**
> “Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.”

**Model Output:**
```
The 19-year-old singer was caught on camera being escorted out of the store by security guards.

The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back.

Scroll down for video

Shoplifting: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today (pictured)

The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back

The singer was also wearing a pair of black-rimmed glasses, a black jacket, black jeans and black sandals.

She was carrying a pair of black and white striped gloves and a small black bag.
```

**Human-Written Prompt:**
> “For today’s homework assignment, please describe the reasons for the US Civil War.”

**Model Output:**
```
By Donny Ferguson

It is easy to identify why the Civil War happened, because so many people and so many books and so much television and films tell us that it was the cause, that it has something to do with race or economics or religion. There is much agreement that it was essentially a war of slavery on behalf of capitalism, about a century of slavery. But that’s not what most people think of when they think of the Civil War. Many people think of the war as about states rights. You’re not wrong about that, but it’s kind of misleading to say that the Civil War was a conflict between states’ rights and federalism. So let’s try again. What’s the idea that has inspired so many people to go onto the battlefields of Gettysburg and Antietam and Gettysberg and Petersburg and Fredericksburg? The American idea of the republic--a notion of limited government--is a great part of the history.
```

## Fine-Tuning GPT-2
For this assignment, your task is to **fine-tune** a released version of GPT-2 on two datasets: a text adventure set that we give you, and on a dataset *of your own choosing*. Preparing your own dataset will involve you downloading/cleaning text from the internet, or creating the dataset yourself. Remember to create the usual train/dev/test split! Include your dataset along with your submission on Gradescope, and describe the process of designing your dataset in your report.

The provided notebook generates text by randomly sampling from the distribution over the next word given to us by the model. One extension that we commonly use to get better generations is to truncate the distribution to only consider the top $k$ words. This is called **top-k** sampling and the value of $k$ is a hyperparameter that we can choose through the process of our experiments. This strategy intuitively makes sense, since words at the bottom of the distribution (i.e. with low and near-zero probabilities) probably won't cause for better generation quality and we're better off re-distributing the probability mass to more likely candidates. 

### Tasks
1. Fine-tune a GPT-2 model (any size is fine, but larger sizes are more fun!) on the provided text-adventure dataset using the example code. Note that training will take a while - training for one epoch using the default parameters takes approximately 25 min. Report the perplexities before and after fine-tuning and describe the trends of perplexity in your report. Generate a few samples before moving on to get a feel for the model. 

2. Repeat 1) using a dataset you put together. Similarly, report perplexities before and after and a few generated samples.

3. The skeleton code uses **top-$k$** sampling, with the $k$ parameter set to be 50. Experiment with different sampling strategies to generate text and include examples, as well as your own qualitative assessment of the outputs in your report.
   
4. Instead of having a leaderboard for this assignment, we will be awarding extra credit to the most subjectively interesting/funny/impressive generations. Submit your favorite 150-token generation as a PDF document on Gradescope to participate!

<!-- List the materials from the header -->
{% if page.materials %}
<div class="alert alert-info">
You can use the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>
</div>
{% endif %}

## Report

1. Summarize fine-tuning results on the provided text-adventure dataset, including perplexities before fine-tuning and afterwards. Comment on the trend you observe. Include a few sample generations. 

2. Describe the process of how you obtained your personal dataset, including any additional pre/post-processing steps that occurred. Comment on any difficulties that arose, and compare your data to the provided dataset, thinking about how you expect model results to change. 

3. Summarize fine-tuning results on your personal dataset, including perplexities before fine-tuning and afterwards. Comment on how this trend differs from the results of Task 1. Include a few sample generations.

4. Experiment with different sampling strategies, including examples of the data your model produces. Include a brief analysis of the obtained results. 

## Language Model Bias
One problem with machine learning models that are trained on large internet-based text corpora is that they exhibit biases that exist in the training data, for example gender bias. In this task, you will get the chance to uncover some of these biases on your own. Using a masked language model demo developed by our TA Sihao [(found here)](http://dickens.seas.upenn.edu:4001/), explore the following questions. In order to use the demo, you need to copy the prompt into the “Mandatory sentence..” box, and keep the default selection as “per-token independent selections”. In your report, include observations in response to the following mini exploration tasks. 

### Exploration Tasks
1. Consider the prompt: `The @ was on his way to lunch.` What are the top 5 most likely professions that the language model predicts?

2. Now consider: `The @ was on her way to lunch.` What are the top 5 most likely professions that the language model predicts? How do these compare to the previous list?

3. Try doing the opposite - choose some profession, and mask the pronoun. For professions that have a clear male/female counterpart (i.e. nurse/doctor, actor/actress, etc.), note whether the predicted pronoun aligns with your notion of “male/female” roles (notice how easily we exhibit these same biases as well…). Could you find professions for which the pronoun predictions are less evidently biased?

    Now, try to change the main structure of the sentence (i.e. move it away from the lunch theme) to encourage the model to give different pronoun predictions for the same profession. What do these results tell you? Is this easy or difficult to do? Does your setup work for different professions? 

4. As a final step, explore biased pronoun prediction outside the scope of professions, for example in the scope of activities (or whatever else interests you). Are there activities you expect to be more gender-neutral than they actually are based on the language model predictions? Are there any particularly surprising examples that stood out to you? Experiment to the degree you find interesting, and briefly summarize your findings in the report.

This is an open problem in natural language processing research: mitigating the uncontrollability of language models.

## Deliverables 
<div class="alert alert-warning" markdown="1">
Here are the deliverables that you will need to submit:
* Code (as a downloaded Colab notebook)
* Your compiled dataset, including train/dev/test splits
* PDF report
* One favourite 150-token generation sample
</div>

## Recommended readings

<table>
   {% for publication in page.readings %}
    <tr>
      <td>
	{% if publication.url %}
		<a href="{{ publication.url }}">{{ publication.title }}.</a>
        {% else %}
		{{ publication.title }}.
	{% endif %}
	{{ publication.authors }}.
	{{ publication.venue }}  {{ publication.year }}.

	{% if publication.abstract %}
	<!-- abstract button -->
	<a data-toggle="modal" href="#{{publication.id}}-abstract" class="label label-success">Abstract</a>
	<!-- /.abstract button -->
	<!-- abstract content -->
	<div id="{{publication.id}}-abstract" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="{{publication.id}}">
    <div class="modal-dialog" role="document">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
          <h4 class="modal-title" id="{{publication.id}}">{{publication.title}}</h4>
        </div><!-- /.modal-header -->
        <div class="modal-body">
        {{publication.abstract}}
        </div><!-- /.modal-body -->
	</div><!-- /.modal-content -->
	</div><!-- /.modal-dialog -->
	</div><!-- /.abstract-content -->
	{% endif %}
		{% if publication.bibtex %}
	<!-- bibtex button -->
	<a data-toggle="modal" href="#{{publication.id}}-bibtex" class="label label-default">BibTex</a>
	<!-- /.bibtex button -->
	<!-- bibtex content -->
	<div id="{{publication.id}}-bibtex" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="{{publication.id}}">
    <div class="modal-dialog" role="document">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
          <h4 class="modal-title" id="{{publication.id}}">{{publication.title}}</h4>
        </div><!-- /.modal-header -->
        <div class="modal-body">
 	   <pre>{{publication.bibtex}}
           </pre>
        </div><!-- /.modal-body -->
	</div><!-- /.modal-content -->
	</div><!-- /.modal-dialog -->
	</div><!-- /.bibtex-content -->
	{% endif %}
</td></tr>
  {% endfor %}
</table>


