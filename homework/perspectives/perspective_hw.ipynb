{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perspectrum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-7aBAtpDG6U",
        "colab_type": "text"
      },
      "source": [
        "# Perspective Relevance and Stance Classificaiton\n",
        "## CIS 530 Homework 11 - Spring 2020\n",
        "Arguments play an important role in understanding controversial topics. For instance, watching debates over an controversial topic is arguably the most efficient way of learning about different perspectives on the matter. However, in real life, information around a topic (e.g. from news publishers) is usually organized in a limited and repetitive way, such that one will not be able to see a variety of perspectives from a diverse background.\n",
        "\n",
        "With the goal of \"showing diverse persepctives with respect to a controversial topic\", one of your TAs built an argument search engine called [PerspectroScope](https://perspectroscope.seas.upenn.edu/). Given a controversial claim as input, the search engine will look for potential arguments on the open web, and use classifiers trained on a dataset called [Perspectrum](https://cogcomp.seas.upenn.edu/perspectrum/), to decide whether each potential argument is indeed relevant and is supporting/refuting the claim. \n",
        "\n",
        "In this homework, we will be using BERT, A powerful and popular Contextual Neural Language Model, to tackle two sentence pair classification tasks that constituates the \"PerspectroScope\" argument search engine.\n",
        "1. Given a claim and an sentence, classify whether the sentence presents a **relevant perspective** to the claim.\n",
        "2. Given a claim and a sentence of relevant perspective, classify whether the perspective **supports or refutes** the claim.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WisUnpKqPW5x",
        "colab_type": "text"
      },
      "source": [
        "## **Part I:** Relevance Classification with BERT fine-tuning\n",
        "\n",
        "#### But first...What is fine-tuning? \n",
        "Fine-tuning is a process to take a machine learning model that has already been trained for a given task/objective, and further train the model with a second similar task/objective. \n",
        "\n",
        "#### Why do we need to fine-tune BERT?\n",
        "BERT is trained with the Masked Language Modeling objective -- a similar but slightly more sophisticated learning objective to the neural language models you have previously seen in class. Here is a nice [demo](https://demo.allennlp.org/masked-lm?text=The%20doctor%20ran%20to%20the%20emergency%20room%20to%20see%20%5BMASK%5D%20patient.) that demonstrates how it works. Your TA also built [a more powerful but less fancy version](http://dickens.seas.upenn.edu:4001/%20%20/I%20love%20%40%20chocolate/perToken). In case you are interested in seeing how BERT works, please try these two demos out. \n",
        "\n",
        "The takeway is that, the Masked LM objective gives BERT the capability of general \"language understanding\". However, without fine-tuning, BERT is not equipped with the \"domain knowledge\" of the speicific tasks you are interested in solving. So you need to initialize your model with pretrained BERT embedding, and further train it with labeled data specific to the task. \n",
        "\n",
        "So far everything looks very similar to the Neural LM homework, where you initialize the model with pretrained word embeddings (e.g. GloVe, Skip-gram). But here's one important difference, which is also one of the reasons why BERT is so powerful: When you fine-tune BERT, you not only fine-tune the last layers, but the entire BERT model's weights are updated. On contrary, when you use word embedding, you don't actually update the part of the network that was used to train the word embeddings. This makes BERT more expressive, and easier to adapt to the task-specific supervision provided during fine-tuning. \n",
        "\n",
        "We will be using the [transformer](https://github.com/huggingface/transformers) package developed by Huggingface, based on PyTorch. It is the most popular library for BERT and other transformer-based language models like GPT-2. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrm0rxUR3MMj",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in Runtime > Change runtime type before running this Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emdQC05Z2H8W",
        "colab_type": "text"
      },
      "source": [
        "### Installing the HuggingfaceðŸ¤— transformer package + other required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5zqkZYt2Jg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "os.chdir('/content/transformers')\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "!pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdUHfAX97ARc",
        "colab_type": "text"
      },
      "source": [
        "### Import the important packages that we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZo2Ls7I5Dmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTIMhxNC6aLA",
        "colab_type": "text"
      },
      "source": [
        "### Mount your google drive \n",
        "\n",
        "We will be saving trained checkpoints on your Google Drive so that they can be accessed even if the Colab session dies. Make sure to login with your UPenn credentials, as you will be saving several gigabytes of data, and Penn gives you unlimited Drive storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrlKyHzY6zq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyvLholz7VHc",
        "colab_type": "text"
      },
      "source": [
        "### Download the PERSPECTRUM dataset\n",
        "Note that with the default code, the files are not saved in your google drive, which means they will get deleted after the session close. You can either re-run this cell for each new colab session, or you can save it to the mounted drive at `/content/drive`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOhxsOGy7cK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dir = '/content/'\n",
        "\n",
        "# Perspectrum Training Set\n",
        "!wget -nc -P {dataset_dir} https://raw.githubusercontent.com/computational-linguistics-class/computational-linguistics-class.github.io/master/homework/perspectives/perspectrum_train.json\n",
        "\n",
        "# Perspectrum Relevance - Dev/Test set\n",
        "!wget -nc -P {dataset_dir} https://raw.githubusercontent.com/computational-linguistics-class/computational-linguistics-class.github.io/master/homework/perspectives/perspectrum_relevance_dev.json\n",
        "!wget -nc -P {dataset_dir} https://raw.githubusercontent.com/computational-linguistics-class/computational-linguistics-class.github.io/master/homework/perspectives/perspectrum_relevance_test_no_label.json\n",
        "\n",
        "# Perspectrum Stance - Dev/Test set\n",
        "!wget -nc -P {dataset_dir} https://raw.githubusercontent.com/computational-linguistics-class/computational-linguistics-class.github.io/master/homework/perspectives/perspectrum_stance_dev.json\n",
        "!wget -nc -P {dataset_dir} https://raw.githubusercontent.com/computational-linguistics-class/computational-linguistics-class.github.io/master/homework/perspectives/perspectrum_stance_test_no_label.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGB5MH22B1dH",
        "colab_type": "text"
      },
      "source": [
        "### Load the dataset and see what it looks like\n",
        "\n",
        "For now let's first load the training dataset and see what it looks like. We will worry about the dev/test sets later...\n",
        "\n",
        "**Note**: In practice you should NEVER look at the training data you are working with, when there's a dev set available. This is to prevent one from designing models that leverage the \"dataset artifacts\" of the training data.  However in this case, we've done negative sampling for you on dev/test set, so they are already in a different format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaBayxnYB-7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open(os.path.join(dataset_dir, 'perspectrum_train.json')) as fin:\n",
        "    train_set = json.load(fin)\n",
        "\n",
        "print(type(train_set))\n",
        "print(\"Number of claims in training set: {}\".format(len(train_set)))\n",
        "print(\"Here's how one of the example looks like: {}\".format(json.dumps(train_set[100])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T16oSXXxGInG",
        "colab_type": "text"
      },
      "source": [
        "So the training set contains a list of claims. Under each claim, there are a few relavant \"perspectives\" either support or refute the claim, which will serve as **positive examples** for training. Note that **we don't have negative examples provided**. This will be the case for most datasets you meet in real life. \n",
        "\n",
        "Now let's randomly sample negative perspectives (i.e. perspectives NOT related to the given claim). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thxwPQ84FFrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def negative_sample(train_set, claim_id, claim_text, sample_size):\n",
        "    \"\"\"\n",
        "    Given a perspective (A dictionnary with keys \"id\" and \"text\"), randomly sample {sample_size} negative examples from the dataset. E.g. get a perspective from a different claim\n",
        "    \"\"\"\n",
        "    # Each perspective object in the list should be a dictionary with two keys \"id\", \"text\".\n",
        "    other_examples = [ex for ex in train_set if ex[\"cid\"] != claim_id]\n",
        "    \n",
        "    negative_examples = []\n",
        "    for i in range(sample_size):\n",
        "        rand_claim = random.choice(other_examples)\n",
        "        all_persps = rand_claim[\"perspective_for\"] + rand_claim[\"perspective_against\"]\n",
        "        random_persp = random.choice(all_persps)\n",
        "        negative_examples.append(random_persp)\n",
        "    \n",
        "    return negative_examples\n",
        "\n",
        "training_sentence_pairs = []\n",
        "\n",
        "for claim in train_set:\n",
        "    positive_perspectives = claim[\"perspective_for\"] + claim[\"perspective_against\"]\n",
        "    \n",
        "    # We keep the number of negative examples equal to positive, so that we will have a balanced training set\n",
        "    negative_perspectives = negative_sample(train_set, claim['cid'], claim['claim_text'], len(positive_perspectives)) \n",
        "    \n",
        "    for persp in positive_perspectives:\n",
        "        training_sentence_pairs.append({\n",
        "            \"claim_id\": claim[\"cid\"],\n",
        "            \"claim_text\": claim[\"claim_text\"],\n",
        "            \"perspective_id\": persp[\"id\"],\n",
        "            \"perspective_text\": persp[\"text\"],\n",
        "            \"label\": True\n",
        "        })\n",
        "\n",
        "    for persp in negative_perspectives:\n",
        "        training_sentence_pairs.append({\n",
        "            \"claim_id\": claim[\"cid\"],\n",
        "            \"claim_text\": claim[\"claim_text\"],\n",
        "            \"perspective_id\": persp[\"id\"],\n",
        "            \"perspective_text\": persp[\"text\"],\n",
        "            \"label\": False\n",
        "        })\n",
        "\n",
        "print(\"Number of claim-perspective sentence pairs for training: {}\".format(len(training_sentence_pairs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7SQ_wPTCvZK",
        "colab_type": "text"
      },
      "source": [
        "Now it would be a good time to load our dev/test examples, which are already organized in the same sentence pair format as what you just did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY1epL37C1v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(dataset_dir, 'perspectrum_relevance_dev.json')) as fin:\n",
        "    dev_sentence_pairs = json.load(fin)\n",
        "\n",
        "with open(os.path.join(dataset_dir, 'perspectrum_relevance_test_no_label.json')) as fin:\n",
        "    test_sentence_pairs = json.load(fin)\n",
        "\n",
        "print(\"Number of claim-perspective sentence pairs in dev set: {}\".format(len(dev_sentence_pairs)))\n",
        "print(\"Number of claim-perspective sentence pairs in test set: {}\".format(len(test_sentence_pairs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcu0HdxgE7i9",
        "colab_type": "text"
      },
      "source": [
        "### Load Pretrained BERT Model\n",
        "For the sake of running time and memory limit, we will be using a mini version of BERT, which consists of 4 transformer layers (as opposed to 12 in the base version of BERT). For the leaderboard, feel free to use a larger size BERT to achieve better performance. \n",
        "\n",
        "You can search for the available models [here](https://huggingface.co/models?search=bert).\n",
        "\n",
        "You can find more examples of different use cases for BERT in the transformer github repo README -- https://github.com/huggingface/transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7YkHAIjCu4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import InputExample\n",
        "from transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                          BertForSequenceClassification, BertTokenizer)\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "import tqdm\n",
        "\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "\n",
        "bert_model_type = 'google/bert_uncased_L-4_H-256_A-4'   # Specs of BERT models with different sizes can be found at https://github.com/google-research/bert/\n",
        "                                                        # You can experiment models with different sizes, to see how it affects performance. \n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(bert_model_type)\n",
        "config = BertConfig.from_pretrained(bert_model_type)\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs-R81c66DoJ",
        "colab_type": "text"
      },
      "source": [
        "### Convert examples to BERT input features\n",
        "Much like every other neural network. You need to (1) tokenize your input sentences (2) Have a vocabulary/dictionary and convert each token to a vector/tensor. Luckily BERT offers a very nice set of interfaces, through which you can do these steps easily.\n",
        "\n",
        "In this homework we provide this function to you. However, in case you would like to use BERT in the future, it is really important to understand BERT's input format and the word-piece tokenziation strategy that BERT adopts. Here are a few resources that we suggest -- \n",
        "\n",
        "1. The [\"What is BERT\" section](https://github.com/google-research/bert#what-is-bert) in the official BERT code repo by Google\n",
        "2. Section 3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBUds7v6V9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "relevance_label_mapping = {\n",
        "    True: 1,\n",
        "    False: 0\n",
        "} # If you are working on stance classification, create a different label mapping\n",
        "\n",
        "def convert_sentence_pair_to_tensor_input(sentence_pairs, label_mapping):\n",
        "\n",
        "    # STEP 1: convert each sentence \n",
        "    input_examples = []\n",
        "    for pair in sentence_pairs:\n",
        "        current_label = pair[\"label\"] if \"label\" in pair else False\n",
        "        input_examples.append(\n",
        "            InputExample(guid=\"\", # We don't really need this\n",
        "                         text_a=pair[\"claim_text\"], \n",
        "                         text_b=pair[\"perspective_text\"], \n",
        "                         label=label_mapping[current_label])\n",
        "        )\n",
        "\n",
        "    label_list = [val for _, val in label_mapping.items()]\n",
        "\n",
        "    features = convert_examples_to_features(input_examples,\n",
        "                                                   tokenizer,\n",
        "                                                   label_list=label_list,\n",
        "                                                   max_length=128,  \n",
        "                                                   output_mode=\"classification\")\n",
        "    \n",
        "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, attention_mask, token_type_ids, labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQHIh7iDMnEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = convert_sentence_pair_to_tensor_input(training_sentence_pairs, relevance_label_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrKngYELCPal",
        "colab_type": "text"
      },
      "source": [
        "### Choose your hyperparameters + model output directory\n",
        "Before we get into training, we need to set our hyperparameters, e.g. Learning rates, mini-batch size for training/testing, etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxxxJxVkCOwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HYPER_PARAMS = {\n",
        "    \"num_training_epoch\": 3,\n",
        "    \"learning_rate\": 3e-5,        # Suggested values -- [1e-5, 3e-5, 5e-5]\n",
        "    \"training_batch_size\": 16,    # Suggested values -- [16, 32]\n",
        "    \"eval_batch_size\": 8,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"num_warmup_steps\": 0.1\n",
        "}\n",
        "\n",
        "model_output_dir = \"/content/drive/\" # Model + prediction results will be saved to your GDrive, \n",
        "                                     # so you don't lose them after session closes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42-9VxM2GizV",
        "colab_type": "text"
      },
      "source": [
        "### Fine-tune BERT model\n",
        "\n",
        "Remember NOT to re-run this cell multiple times, without re-initializing the BERT model. Multiple runs will effectively train your model with more epochs than you intended!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWvAzdYDDZ3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "\n",
        "bert_model.to('cuda')\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, \n",
        "                              sampler=train_sampler, \n",
        "                              batch_size=HYPER_PARAMS[\"training_batch_size\"])\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), \n",
        "                  lr=HYPER_PARAMS['learning_rate'], \n",
        "                  correct_bias=False)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=HYPER_PARAMS['num_warmup_steps'], \n",
        "                                            num_training_steps=len(train_dataloader))\n",
        "\n",
        "\n",
        "global_step = 0\n",
        "tr_loss = 0.0\n",
        "bert_model.zero_grad()\n",
        "bert_model.train()\n",
        "\n",
        "for epc in range(HYPER_PARAMS[\"num_training_epoch\"]):\n",
        "    print(\"Epoch #{}: \\n\".format(epc))\n",
        "    epoch_iterator = tqdm.notebook.tqdm(train_dataloader, desc=\"Training Steps\")\n",
        "    avg_loss_over_epoch = []\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'token_type_ids': batch[2],\n",
        "                  'labels': batch[3]}\n",
        "\n",
        "        outputs = bert_model(**inputs)\n",
        "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), HYPER_PARAMS[\"max_grad_norm\"])\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        bert_model.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C731cQRQ9BxZ",
        "colab_type": "text"
      },
      "source": [
        "### Save the fine-tuned model\n",
        "It is good practice to save your tokenizer + config for BERT at the same location, for best reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa8blsqb9FMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# This is where we mounted your google drive. \n",
        "# You might need to re-mount it if your session was closed half way through\n",
        "output_dir = \"/content/drive/My Drive/cis530_perspective_hw/relevance_model/\" \n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "bert_model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "config.save_pretrained(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofvQ_kkHjdq",
        "colab_type": "text"
      },
      "source": [
        "### Test if you can load the model back!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo96_-F4Hm2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Don't forget to move your model to GPU/CUDA after loading back from disk!\n",
        "bert_model = bert_model.to(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_xGg1Tg8JdW",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the fine-tuned model on dev set\n",
        "Now we want to know how good our model is. Let's test it on the dev set!\n",
        "\n",
        "We need to go through the same process -- convert sentence pairs into feature vectors/tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJV-9ZQAIzsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Putting this here again, just so you don't forget what it is...\n",
        "relevance_label_mapping = {\n",
        "    True: 1,\n",
        "    False: 0\n",
        "} \n",
        "\n",
        "dev_dataset = convert_sentence_pair_to_tensor_input(dev_sentence_pairs, relevance_label_mapping)\n",
        "\n",
        "# We are not random sampling anymore when evaluating... As we want to keep the order \n",
        "dev_sampler = SequentialSampler(dev_dataset)\n",
        "dev_dataloader = DataLoader(dev_dataset, \n",
        "                            sampler=dev_sampler, \n",
        "                            batch_size=HYPER_PARAMS[\"eval_batch_size\"])\n",
        "\n",
        "predictions = None\n",
        "out_label_ids = None\n",
        "\n",
        "for batch in tqdm.notebook.tqdm(dev_dataloader, desc=\"Evaluating on Dev set...\"):\n",
        "    bert_model.eval()\n",
        "    batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "    inputs = {'input_ids': batch[0],\n",
        "              'attention_mask': batch[1],\n",
        "              'token_type_ids': batch[2],\n",
        "              'labels': batch[3]}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        logits = outputs[1] # This is 1x2 tensor, containing scores for both labels \n",
        "\n",
        "    if predictions is None:\n",
        "        predictions = logits.detach().cpu().numpy()\n",
        "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "    else:\n",
        "        predictions = np.append(predictions, logits.detach().cpu().numpy(), axis=0)\n",
        "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "# whichever label gets higher score, we will predict that label\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "\n",
        "# We will simply use accuracy as our measure here \n",
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "acc = accuracy(predictions, out_label_ids)\n",
        "\n",
        "print(\"The accuracy on dev set = {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLPicglpM4I1",
        "colab_type": "text"
      },
      "source": [
        "The TAs were able to get around 75-80% accuracy on the dev set, with the provided set of parameters and model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNIV7pxxNJES",
        "colab_type": "text"
      },
      "source": [
        "### Now it's your turn - Evaluate on the test data, and submit your results\n",
        "\n",
        "This should be almost identical to what we just did for the dev set. \n",
        "\n",
        "Please download the `relevance_test_predictions.txt` and follow guide on the homework webpage to make a submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpg5Cx4uOBkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_on_test_set():\n",
        "    \"\"\"\n",
        "    Return a list of 0/1 prediction for each test example, in sequential order.\n",
        "    Please use the same label mapping as we have so far.\n",
        "    1 = True (Relevant)\n",
        "    0 = False (Not relevant)\n",
        "    \"\"\"\n",
        "\n",
        "    test_dataset = convert_sentence_pair_to_tensor_input(test_sentence_pairs, relevance_label_mapping)\n",
        "    \n",
        "    # TODO: fill the rest here\n",
        "    \n",
        "    return list_of_predictions\n",
        "\n",
        "\n",
        "# Feel free to change the save location as you like,\n",
        "# but please keep the file name as \"relevance_test_predictions.txt\"\n",
        "# So that the autograder will know what file to look for...\n",
        "test_result_output_path = \"/content/drive/My Drive/cis530_perspective_hw/relevance_test_predictions.txt\"\n",
        "\n",
        "test_predictions = predict_on_test_set()\n",
        "\n",
        "with open(test_result_output_path, 'w') as fout:\n",
        "    for pred in test_predictions: \n",
        "        fout.write(\"{}\\n\".format(int(pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUw8oHtcS7Qv",
        "colab_type": "text"
      },
      "source": [
        "## **Part II:** DIY for stance classification (Optional, Extra Credit)\n",
        "\n",
        "Now that you are becoming an expert for BERT (hopefully), why don't you try to tackle our second task -- stance classification, to predict whether a relevant perspective is eihter **supporting or refuting** the claim.\n",
        "\n",
        "Since this is a different task, you will be generating positive and negative sentence pairs in a slightly different way. Sepcifically --\n",
        "\n",
        "1.   In `perspectrum_train.json`, for each given claim, both supporting and refuting perspectives have been given to you. So you don't need to do negative sampling. Instead you should take the claim + \"supporting\" perspective as positive sentence pair and claim with \"refuting\" perspective as negative pair.   \n",
        "\n",
        "2.   The task assumes that for every input claim-perspective pair, the perspective is relevant to the claim. So when generating training pairs, you should make sure of that.\n",
        "\n",
        "But once you have generated sentence pairs from the training data, the training/evaluation procedure should be almost identical. For the most part you will be re-using code that we just went through.\n",
        "\n",
        "### **What you need to submit**:\n",
        "Like what we did for the perspective relevance classification, we want to you train a model and write your stance classification predictions on the test data to a file named `stance_test_predictions.txt`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPwCjw17T3kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(dataset_dir, 'perspectrum_train.json')) as fin:\n",
        "    train_set = json.load(fin)\n",
        "\n",
        "# TODO: start from here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCYGLc1Wgs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The dev and test sets are, again, made into sentence pairs format for you already\n",
        "with open(os.path.join(dataset_dir, 'perspectrum_stance_dev.json')) as fin:\n",
        "    dev_sentence_pairs = json.load(fin)\n",
        "\n",
        "with open(os.path.join(dataset_dir, 'perspectrum_stance_test_no_label.json')) as fin:\n",
        "    test_sentence_pairs = json.load(fin)\n",
        "\n",
        "print(\"Number of claim-perspective sentence pairs in dev set: {}\".format(len(dev_sentence_pairs)))\n",
        "print(\"Number of claim-perspective sentence pairs in test set: {}\".format(len(test_sentence_pairs)))\n",
        "\n",
        "stance_label_mapping = {\n",
        "    \"support\": 1,\n",
        "    \"refute\": 0\n",
        "} \n",
        "\n",
        "# TODO: start from here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}